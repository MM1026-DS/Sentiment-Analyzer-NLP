{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"K8 Reviews v0.2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Good but need updates and improvements</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Worst mobile i have bought ever, Battery is dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>when I will get my 10% cash back.... its alrea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>The worst phone everThey have changed the last...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                             review\n",
       "0          1             Good but need updates and improvements\n",
       "1          0  Worst mobile i have bought ever, Battery is dr...\n",
       "2          1  when I will get my 10% cash back.... its alrea...\n",
       "3          1                                               Good\n",
       "4          0  The worst phone everThey have changed the last..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = dataset['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['review']=[word.lower() for word in dataset['review'] ]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reviews_up = dataset['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Reviews_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               good but need updates and improvements\n",
       "1    worst mobile i have bought ever, battery is dr...\n",
       "2    when i will get my 10% cash back.... its alrea...\n",
       "3                                                 good\n",
       "4    the worst phone everthey have changed the last...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Reviews_up.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reviews_list = [word.lower() for word in dataset['review'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Reviews_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good but need updates and improvements'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Reviews_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fist I tokkenize in sent tokken\n",
    "\n",
    "Sentence_tokkenize = [nltk.sent_tokenize(sentence) for sentence in Reviews_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"worst mobile i have bought ever, battery is draining like hell, backup is only 6 to 7 hours with internet uses, even if i put mobile idle its getting discharged.this is biggest lie from amazon & lenove which is not at all expected, they are making full by saying that battery is 4000mah & booster charger is fake, it takes at least 4 to 5 hours to be fully charged.don't know how lenovo will survive by making full of us.please don;t go for this else you will regret like me.\"]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sentence_tokkenize[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_tokkenize =[nltk.word_tokenize(words) for words in  Reviews_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good', 'but', 'need', 'updates', 'and', 'improvements']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_tokkenize[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/labsuser/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "nltk.download(\"stopwords\")\n",
    "stopword_all = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tagged_words = [nltk.pos_tag(words_pos) for words_pos in words_tokkenize ]\n",
    "# Tagged_words_chunk = [nltk.ne_chunk(Tagged_words_chunk) for Tagged_words_chunk in Tagged_words] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14675"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            \n",
    "   # Noun tagging\n",
    "Noun_tag_pos =[]\n",
    "for j in range(len(Tagged_words)):\n",
    "    for noun_tg in Tagged_words[j]:\n",
    "        if noun_tg[1]==\"NN\" or noun_tg[1]==\"NNP\" or noun_tg[1]==\"NNPS\" or noun_tg[1]==\"NNS\":\n",
    "            Noun_tag_pos.append(noun_tg)\n",
    "               \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mobile', 'NN')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Noun_tag_pos[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "Rg_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function creation for lemmatiztio\n",
    "\n",
    "# def stopword_punchremo(sent):\n",
    "#     lem_word =[]\n",
    "#     import nltk\n",
    "#     nltk.download('wordnet')\n",
    "#     from nltk.stem import WordNetLemmatizer \n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     for j in range(len(sent)):\n",
    "#         b = [lemmatizer.lemmatize(word,'v') for word in sent[j] if not word in set(stopword_all)]\n",
    "#         lem_word.append(b) \n",
    "#         print(lem_word)\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def stopword_punchremo(sent):\n",
    "#     lem_word =[]\n",
    "#     import nltk\n",
    "#     nltk.download('wordnet')\n",
    "#     from nltk.stem import WordNetLemmatizer \n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     for j in range(len(sent)):\n",
    "#         b = [lemmatizer.lemmatize(word,'v') for word in sent[j] ]\n",
    "#         lem_word.append(b) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lem_word =[]\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# from nltk.stem import WordNetLemmatizer \n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# for j in range(len(words_tokkenize)):\n",
    "#     b = [lemmatizer.lemmatize(word,'v') for word in words_tokkenize[j] if not word in set(stopword_all)]\n",
    "#     lem_word.append(b) \n",
    "#     print(lem_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pun_removal(sto:\n",
    "#     from nltk.tokenize import RegexpTokenizer\n",
    "#     Rg_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#     new_sent = Rg_tokenizer.tokenize(stopword_punchremo(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pun_removal(sent):\n",
    "    \n",
    "#     from nltk.tokenize import RegexpTokenizer\n",
    "#     Rg_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#     new_sent = Rg_tokenizer.tokenize(sent)\n",
    "#     k.append(new_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  from nltk.tokenize import RegexpTokenizer\n",
    "# Rg_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# for i in range(len(Sentence_tokkenize)):\n",
    "#     new_sent = Rg_tokenizer.tokenize(\"I AM HERO,YOU ARE,NO HERO\")  \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import string\n",
    "# x = [' '.join(c for c in s if c not in string.punctuation) for s in words_tokkenize]\n",
    "# print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/labsuser/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# lemmatize and removing stopwords and cleaning \n",
    "\n",
    "import re\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "corpus = []\n",
    "for i in range(0, 14675):\n",
    "  review = re.sub('[^a-zA-Z]', ' ', dataset['review'][i])\n",
    "  review = review.lower()\n",
    "  review = review.split()\n",
    "  all_stopwords = stopwords.words('english')\n",
    "  review = [lemmatizer.lemmatize(word,'v') for word in review if not word in set(all_stopwords)]\n",
    "  review = ' '.join(review)\n",
    "  corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing all punctuations\n",
    "import string\n",
    "corpus_upd= [''.join(c for c in s if c not in string.punctuation) for s in corpus]\n",
    "# print(corpus_upd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokken_1=[nltk.word_tokenize(e) for e in corpus_upd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good', 'need', 'update', 'improvements']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokken_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['good', 'need', 'update', 'improvements'], ['worst', 'mobile', 'buy', 'ever', 'battery', 'drain', 'like', 'hell', 'backup', 'hours', 'internet', 'use', 'even', 'put', 'mobile', 'idle', 'get', 'discharge', 'biggest', 'lie', 'amazon', 'lenove', 'expect', 'make', 'full', 'say', 'battery', 'mah', 'booster', 'charger', 'fake', 'take', 'least', 'hours', 'fully', 'charge', 'know', 'lenovo', 'survive', 'make', 'full', 'us', 'please', 'go', 'else', 'regret', 'like']]\n"
     ]
    }
   ],
   "source": [
    "processed_docs = []\n",
    "\n",
    "for doc in word_tokken_1:\n",
    "    processed_docs.append((doc))\n",
    "\n",
    "print(processed_docs[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n= 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(43, 1), (106, 1), (107, 1)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus[21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 2 (\"amazon\") appears 1 time.\n",
      "Word 3 (\"backup\") appears 1 time.\n",
      "Word 4 (\"biggest\") appears 1 time.\n",
      "Word 5 (\"buy\") appears 1 time.\n",
      "Word 6 (\"charge\") appears 1 time.\n",
      "Word 7 (\"charger\") appears 1 time.\n",
      "Word 8 (\"discharge\") appears 1 time.\n",
      "Word 9 (\"drain\") appears 1 time.\n",
      "Word 10 (\"else\") appears 1 time.\n",
      "Word 11 (\"even\") appears 1 time.\n",
      "Word 12 (\"ever\") appears 1 time.\n",
      "Word 13 (\"expect\") appears 1 time.\n",
      "Word 14 (\"fake\") appears 1 time.\n",
      "Word 15 (\"full\") appears 2 time.\n",
      "Word 16 (\"fully\") appears 1 time.\n",
      "Word 17 (\"get\") appears 1 time.\n",
      "Word 18 (\"go\") appears 1 time.\n",
      "Word 19 (\"hell\") appears 1 time.\n",
      "Word 20 (\"hours\") appears 2 time.\n",
      "Word 21 (\"idle\") appears 1 time.\n",
      "Word 22 (\"internet\") appears 1 time.\n",
      "Word 23 (\"know\") appears 1 time.\n",
      "Word 24 (\"least\") appears 1 time.\n",
      "Word 25 (\"like\") appears 2 time.\n",
      "Word 26 (\"mah\") appears 1 time.\n",
      "Word 27 (\"make\") appears 2 time.\n",
      "Word 28 (\"please\") appears 1 time.\n",
      "Word 29 (\"put\") appears 1 time.\n",
      "Word 30 (\"regret\") appears 1 time.\n",
      "Word 31 (\"say\") appears 1 time.\n",
      "Word 32 (\"take\") appears 1 time.\n",
      "Word 33 (\"us\") appears 1 time.\n",
      "Word 34 (\"use\") appears 1 time.\n",
      "Word 35 (\"worst\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "document_num = 1\n",
    "bow_doc_x = bow_corpus[document_num]\n",
    "\n",
    "for i in range(len(bow_doc_x)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_x[i][0], \n",
    "                                                     dictionary[bow_doc_x[i][0]], \n",
    "                                                     bow_doc_x[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
    "                                   num_topics = 10, \n",
    "                                   id2word = dictionary,                                    \n",
    "                                   passes = 10,\n",
    "                                   workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.032*\"bad\" + 0.023*\"dual\" + 0.023*\"quality\" + 0.020*\"better\" + 0.020*\"front\" + 0.017*\"processor\" + 0.017*\"gb\" + 0.016*\"ram\" + 0.015*\"super\" + 0.014*\"rear\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.139*\"nice\" + 0.071*\"price\" + 0.068*\"best\" + 0.044*\"excellent\" + 0.030*\"one\" + 0.029*\"range\" + 0.027*\"money\" + 0.024*\"worth\" + 0.023*\"feature\" + 0.022*\"value\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.041*\"quality\" + 0.039*\"awesome\" + 0.035*\"great\" + 0.028*\"performance\" + 0.021*\"ok\" + 0.018*\"amaze\" + 0.018*\"back\" + 0.017*\"sound\" + 0.016*\"feature\" + 0.016*\"low\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.088*\"charge\" + 0.043*\"use\" + 0.041*\"get\" + 0.026*\"take\" + 0.024*\"drain\" + 0.022*\"time\" + 0.021*\"fast\" + 0.020*\"charger\" + 0.020*\"hours\" + 0.019*\"turbo\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.042*\"issue\" + 0.031*\"update\" + 0.023*\"call\" + 0.021*\"sim\" + 0.021*\"network\" + 0.020*\"problem\" + 0.018*\"g\" + 0.016*\"get\" + 0.015*\"use\" + 0.014*\"even\"\n",
      "\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.036*\"screen\" + 0.033*\"call\" + 0.024*\"feature\" + 0.023*\"cast\" + 0.021*\"hai\" + 0.020*\"record\" + 0.019*\"option\" + 0.018*\"app\" + 0.017*\"available\" + 0.016*\"h\"\n",
      "\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.073*\"buy\" + 0.062*\"worst\" + 0.039*\"problem\" + 0.031*\"poor\" + 0.027*\"bad\" + 0.027*\"waste\" + 0.022*\"money\" + 0.020*\"ever\" + 0.020*\"amazon\" + 0.016*\"return\"\n",
      "\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.081*\"k\" + 0.064*\"note\" + 0.024*\"amazon\" + 0.019*\"service\" + 0.019*\"get\" + 0.018*\"buy\" + 0.013*\"return\" + 0.010*\"better\" + 0.010*\"use\" + 0.010*\"please\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 need\n",
      "1 update\n",
      "2 amazon\n",
      "3 backup\n",
      "4 biggest\n",
      "5 buy\n",
      "6 charge\n",
      "7 charger\n",
      "8 discharge\n",
      "9 drain\n",
      "10 else\n",
      "11 even\n",
      "12 ever\n",
      "13 expect\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 13:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.6163986950975919\n"
     ]
    }
   ],
   "source": [
    "# Compute Coherence Score using c_v\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "coherence_model_sc = CoherenceModel(model=lda_model, texts=word_tokken_1, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_sc.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
